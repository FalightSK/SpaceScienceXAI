{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation='relu', output_activation=None, dropout_rate=0.0):\n",
    "        super(MLPRegressor, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Define activation functions\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif activation == 'elu':\n",
    "            self.activation = nn.ELU()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid activation function: {activation}\")\n",
    "\n",
    "        if output_activation == 'sigmoid':\n",
    "            self.output_activation = nn.Sigmoid()\n",
    "        elif output_activation == 'tanh':\n",
    "            self.output_activation = nn.Tanh()\n",
    "        elif output_activation is None:\n",
    "            self.output_activation = None\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid output activation function: {output_activation}\")\n",
    "\n",
    "        # Create the layers\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(self.activation)\n",
    "            if dropout_rate > 0.0:\n",
    "                layers.append(nn.Dropout(dropout_rate))  # Add dropout after activation\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        if self.output_activation:\n",
    "          layers.append(self.output_activation)\n",
    "\n",
    "\n",
    "        # Combine layers into a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10, val_loader=None, patience=None, verbose=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.train() #set to train mode\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, inputs in enumerate(train_loader):\n",
    "            inputs, labels = inputs[\"features\"].to(device), inputs[\"target\"].to(device)\n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze_(dim=-1), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_loss_history.append(epoch_loss)\n",
    "\n",
    "        if verbose:\n",
    "          print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\", end=\"\")\n",
    "\n",
    "        # Validation loop (if val_loader is provided)\n",
    "        if val_loader:\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs in val_loader:\n",
    "                    inputs, labels = inputs[\"features\"].to(device), inputs[\"target\"].to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs.squeeze_(dim=-1), labels)\n",
    "                    val_loss += loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "            val_loss_history.append(val_loss)\n",
    "            if verbose:\n",
    "                print(f\", Val Loss: {val_loss:.4f}\", end=\"\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if patience is not None:\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    epochs_no_improve = 0\n",
    "                    # Save the best model so far\n",
    "                    best_model_state = model.state_dict()\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if epochs_no_improve >= patience:\n",
    "                      if verbose:\n",
    "                        print(f'\\nEarly stopping after {epoch + 1} epochs!')\n",
    "                      model.load_state_dict(best_model_state)  # Load best model\n",
    "                      return train_loss_history, val_loss_history, best_val_loss\n",
    "\n",
    "        if verbose:\n",
    "          print()\n",
    "        model.train()  # Set back to train mode\n",
    "\n",
    "    if val_loader and patience:\n",
    "      model.load_state_dict(best_model_state)\n",
    "\n",
    "    return train_loss_history, val_loss_history, best_val_loss if val_loader else None\n",
    "\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    target = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs in data_loader:\n",
    "            inputs = inputs[\"features\"].to(device)\n",
    "            outputs = model(inputs)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            target.append(inputs[\"target\"].cpu().numpy())\n",
    "\n",
    "    return predictions, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import TabularDataset, create_dataloader, split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "feature_cols = ['Scalar_B', 'BX_GSE', 'BY_GSM', 'BZ_GSM', 'SW_Temp', 'SW_Density', 'SW_Speed', 'Flow_Pressure', 'E_Field']\n",
    "target_col = 'Dst'\n",
    "context_cols = ['time']\n",
    "numerical_features_info = {k:\"standard\" for k in feature_cols}\n",
    "\n",
    "dataset = TabularDataset(\n",
    "    data_path=r\"./dataset/dataset_log.csv\",\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=target_col,\n",
    "    context_cols=context_cols,\n",
    "    numerical_features_info=numerical_features_info\n",
    ")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = split_data(dataset)\n",
    "train_loader = create_dataloader(train_dataset, batch_size=batch_size)\n",
    "val_loader = create_dataloader(val_dataset, batch_size=512)\n",
    "test_loader = create_dataloader(test_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.9097, Val Loss: 0.9338\n",
      "Epoch 2/100, Loss: 0.8203, Val Loss: 0.8915\n",
      "Epoch 3/100, Loss: 0.7536, Val Loss: 0.7019\n",
      "Epoch 4/100, Loss: 0.6807, Val Loss: 0.6101\n",
      "Epoch 5/100, Loss: 0.6326, Val Loss: 0.5879\n",
      "Epoch 6/100, Loss: 0.6027, Val Loss: 0.5478\n",
      "Epoch 7/100, Loss: 0.6076, Val Loss: 0.5606\n",
      "Epoch 8/100, Loss: 0.5886, Val Loss: 0.5431\n",
      "Epoch 9/100, Loss: 0.5997, Val Loss: 0.5417\n",
      "Epoch 10/100, Loss: 0.5840, Val Loss: 0.5092\n",
      "Epoch 11/100, Loss: 0.5654, Val Loss: 0.5215\n",
      "Epoch 12/100, Loss: 0.5624, Val Loss: 0.5647\n",
      "Epoch 13/100, Loss: 0.5600, Val Loss: 0.5571\n",
      "Epoch 14/100, Loss: 0.5680, Val Loss: 0.5297\n",
      "Epoch 15/100, Loss: 0.5598, Val Loss: 0.4974\n",
      "Epoch 16/100, Loss: 0.5584, Val Loss: 0.6680\n",
      "Epoch 17/100, Loss: 0.5450, Val Loss: 0.5160\n",
      "Epoch 18/100, Loss: 0.5396, Val Loss: 0.5551\n",
      "Epoch 19/100, Loss: 0.5473, Val Loss: 0.5291\n",
      "Epoch 20/100, Loss: 0.5521, Val Loss: 0.5752\n",
      "Epoch 21/100, Loss: 0.5481, Val Loss: 0.5060\n",
      "Epoch 22/100, Loss: 0.5431, Val Loss: 0.5624\n",
      "Epoch 23/100, Loss: 0.5530, Val Loss: 0.5082\n",
      "Epoch 24/100, Loss: 0.5411, Val Loss: 0.5271\n",
      "Epoch 25/100, Loss: 0.5531, Val Loss: 0.4777\n",
      "Epoch 26/100, Loss: 0.5355, Val Loss: 0.4872\n",
      "Epoch 27/100, Loss: 0.5393, Val Loss: 0.5070\n",
      "Epoch 28/100, Loss: 0.5368, Val Loss: 0.5256\n",
      "Epoch 29/100, Loss: 0.5588, Val Loss: 0.4785\n",
      "Epoch 30/100, Loss: 0.5240, Val Loss: 0.5182\n",
      "Epoch 31/100, Loss: 0.5408, Val Loss: 0.5178\n",
      "Epoch 32/100, Loss: 0.5268, Val Loss: 0.5222\n",
      "Epoch 33/100, Loss: 0.5324, Val Loss: 0.4820\n",
      "Epoch 34/100, Loss: 0.5302, Val Loss: 0.4784\n",
      "Epoch 35/100, Loss: 0.5303, Val Loss: 0.4916\n",
      "Early stopping after 35 epochs!\n"
     ]
    }
   ],
   "source": [
    "# Create the MLP model\n",
    "input_size = next(iter(train_loader))['features'].shape[1]\n",
    "hidden_sizes = [64, 32]  # Two hidden layers\n",
    "output_size = 1  # Regression output\n",
    "model = MLPRegressor(input_size, hidden_sizes, output_size, activation='relu', dropout_rate=0.2)\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Train the model\n",
    "train_loss, val_loss, best_val = train_model(model, train_loader, criterion, optimizer, num_epochs=100, val_loader=val_loader, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions, target \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Evaluate the model (e.g., using R-squared)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m r2_score\n",
      "Cell \u001b[0;32mIn[2], line 137\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m    135\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m    136\u001b[0m         predictions\u001b[38;5;241m.\u001b[39mextend(outputs\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m--> 137\u001b[0m         target\u001b[38;5;241m.\u001b[39mappend(\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions, target\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "predictions, target = predict(model, test_loader)\n",
    "\n",
    "\n",
    "# Evaluate the model (e.g., using R-squared)\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(target, predictions)\n",
    "print(f\"R-squared on the test set: {r2:.4f}\")\n",
    "\n",
    "# --- Plotting (optional) ---\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "if val_loader:\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of predicted vs. actual values (optional)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(target, predictions)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs. Predicted Values')\n",
    "# Add a diagonal line for perfect predictions\n",
    "plt.plot([target.min(), target.max()], [target.min(), target.max()], 'k--', lw=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
